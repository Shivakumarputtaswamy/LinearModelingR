---
title: "Linear Modeling in R"
author: "Clay Ford"
date: "October 2014"
output: beamer_presentation
---

## What are Linear Models?

Linear Models are mathematical representations of the process that (_we think_) gave rise to our data.

The model seeks to explain the relationship between a variable of interest, our _response_ or _dependent_ variable, and one or more _predictor_ or _independent_ variables.

Linear models feature _linear parameters_. That means no parameter appears as an exponent or is multiplied or divided by another parameter. Parameters are _additive_.

Linear often refers to straight lines, but linear models can be curved.

## Example of a classic linear model

$$Y_{i} = \beta_{0} +  \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i}$$

- $Y_{i}$ is the response of the $i^{th}$ observation
- $\beta_{0}, \beta_{1}, \beta_{2}$ are coefficients
- $X_{i1}$ and $X_{i2}$ are predictors of the $i^{th}$ observation
- $\epsilon_{i}$ is random error of the $i^{th}$ observation; we assume errors are independent and come from a Normal distribution with mean = 0 and variance $\sigma^{2}$; this implies $Y_{i}$ comes from a Normal distribution with mean $\beta_{0} +  \beta_{1}X_{1} + \beta_{2}X_{2}$ and variance $\sigma^{2}$.

_Building a linear model_ means we propose a linear model and then estimate the parameters and the variance of the error term. Above, this means estimating $\beta_{0}, \beta_{1}, \beta_{2}$ and $\sigma^{2}$. This is what we do in R.

## Matrix representation of linear models

Linear models are often represented as matrices/vectors.

$$Y = X\beta + \epsilon$$

For our previous example:

$$\left( \begin{array}{c} y_{1} \\ y_{2} \\ \dots \\ y_{n} \end{array} \right) =   
  \left( \begin{array}{ccc} 1 & x_{11} & x_{12} \\
                          1 & x_{21} & x_{22} \\
                      \dots & \dots & \dots \\
                      1 & x_{n1} & x_{n2} \end{array} \right) 
  \left( \begin{array}{c} \beta_{0} \\ \beta_{1} \\ \beta_{2} \end{array} \right) +
  \left( \begin{array}{c} \epsilon_{1} \\ \epsilon_{2} \\ \dots \\ \epsilon_{n} \end{array} \right)$$

The column of ones incorporates the intercept term. R makes it easy to visualize this representation.

## Estimating $\mathbf{\beta}$

The best estimate minimizes the sum of squared errors. 2D example: line with intercept $\hat{\beta_{0}}$ and slope $\hat{\beta_{1}}$ with squared and summed distances of red lines (ie, residuals) as small as possible.

```{r, echo=FALSE}
x <- seq(1,10)
set.seed(1)
y <- 3 + 2*x + rnorm(n=10,mean=0,sd=3)
opar <- par(mai=c(1.093333, 1.093333, 0, 0.560000))
plot(x,y, xlim=c(0,10), ylim=c(0,25))
slm <- lm(y ~ x)
abline(slm)
segments(x0=x, y0=slm$fitted, x1=x, y1=y, col="red")
par(opar)

```


## Estimating $\sigma^{2}$

Square and sum the red lines (sum of squared errors, aka residual sum of squares, RSS) and divide by $n - p$. ($p =$ number of coefficients) 

```{r, echo=FALSE}
x <- seq(1,10)
set.seed(1)
y <- 3 + 2*x + rnorm(n=10,mean=0,sd=3)
opar <- par(mai=c(1.093333, 1.093333, 0, 0.560000))
plot(x,y, xlim=c(0,10), ylim=c(0,25))
slm <- lm(y ~ x)
abline(slm)
segments(x0=x, y0=slm$fitted, x1=x, y1=y, col="red")
par(opar)
```

## Before fitting a linear model: import data

Most any kind of data can be read into R. The usual form of the function is `read.x`.

- CSV: `read.csv(file="file.csv")`
- TXT: `read.table(file="file.txt", header=TRUE)`
- Fixed-width: `read.fwf(file="file.dat", widths = c(1,2,3))`

The `foreign` package allows you to read in data from other programs. `library(foreign)`:

- Stata: `read.dta`
- SPSS: `read.spss`

You can also use point-and-click in R Studio: "Import Dataset" button.

## Before fitting a linear model: descriptive stats

Look at descriptive statistics before building a model. Helps you...

- spot missing data
- identify potential errors
- determine if data coded correctly

Two basic functions:

1. `summary(yourdata)`: Statistical summaries of all variables
2. `str(yourdata)`: Structure of data 


## Before fitting a linear model: exploratory plots

Always try to visually examine your data before building a model. Helps you...

- identify potential models
- spot potential outliers or errors
- determine if transformations may be necessary

Three basic plots:

1. `hist(response)`: Histogram of response 
2. `plot(response ~ predictor, data=yourdata)`: scatterplot/boxplots
3. `pairs(yourdata)`: All pairwise scatter plots 

The `car` package has two nice exploratory plotting functions: `scatterplot` and `scatterplotMatrix`.

## Fitting a linear model in R

The basic function is `lm`. The main arguments are `formula` and `data`. 

The "formula" is the linear model expressed in what is called _Wilkinson-Rogers_ notation. 

To fit $Y_{i} = \beta_{0} +  \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i}$ do the following:  
`lm(formula=response ~ var1 + var2, data=yourdata)`

Or more concisely:  
`lm(response ~ var1 + var2, yourdata)`

## Saving and working with a linear model

Results of a linear model can be saved, like so:  
`lm1 <- lm(Y ~ X1 + X2, yourdata)`

`lm1` is a linear model object that contains various quantities of interest.

Use _extractor_ functions to view the different quantities. Common ones are `summary`, `coef`, `residuals`, and `fitted`.

For example, `summary(lm1)`.

Let's go to the R script.

## A closer look at the R model summary

```
Call:
lm(formula = psa ~ volume + weight + age + bph + svi + cap.pen + 
    gleason.score, data = prostate)
```
Repeat of the function call.  Useful if result is saved and then printed later.

```
Residuals:
     Min       1Q   Median       3Q      Max 
-1.88309 -0.46629  0.08045  0.47380  1.53219 
```

Quick check of the distributional (Normal) assumptions of residuals. Median should not be far from 0. Max and Min should be roughly equal in absolute value.

## A closer look at the R model summary

```
Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -0.685796   0.998754  -0.687  0.49409    
volume         0.069454   0.014624   4.749 7.77e-06 ***
...
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

```
- **Estimate**: $\hat{\beta}$
- **Std. Error**: SE of the $\hat{\beta}$ 
- **t value**: statistic for test $H_{0}:\hat{\beta} = 0$ *in the model* (Estimate $\div$ Std. Error)
- **Pr(>|t|)**: p-value of hypothesis test (2-sided)
- **Signif. codes**: indicators of significance; one star means $0.01 < p < 0.05$

## A closer look at the R model summary

```
Residual standard error: 0.7679 on 89 degrees of freedom
Multiple R-squared:  0.5893,  Adjusted R-squared:  0.557 
F-statistic: 18.24 on 7 and 89 DF,  p-value: 7.694e-15
```
- **Residual standard error**: $\hat{\sigma}$; estimate of $\sqrt{\sigma^{2}}$
- **degrees of freedom**: # of obs - # of predictors
- **Multiple R-squared**: measure of model fit
- **Adjusted R-squared**: measure of model fit adjusted for number of parameters
- **F-statistic**: statistic for hypothesis test all coefficients (other than intercept) = 0
- **p-value**: p-value of hypothesis test


## Confidence intervals for $\hat{\beta}$

Confidence intervals allow us to express uncertainty in our estimates.

They tell us about plausible values for parameters that hypothesis tests cannot.

To extract from model in R, use `confint` function. Output includes lower and upper bounds.

Default is 95%, but can be modified using the `level` argument.

The `arm` package provides a nice function called `coefplot` to easily plot confidence intervals of coefficients.

## Confidence intervals for predictions

We can make predictions using our linear model, but there will be uncertainty. We need to calculate that uncertainty.

Two forms of prediction:

1. predictions of mean response (predicted mean PSA given these predictors)
2. predictions of observed response (predicted PSA value for _an individual_ with given predictors)

To make predictions in R, use `predict` function. Output includes fit and lower/upper bounds.

- use argument `interval="confidence"` for mean response
- use argument `interval="prediction"` for observed response

Let's go to R.

## Model specification in R

We mentioned earlier R uses the _Wilkinson-Rogers_ notation for specifying models:

> response variable ~ explanatory variable(s)

The tilde (~) can be read as "is modelled as a function of" or "described by" or "regressed on".

Model formulation is used throughout R in plotting, aggregation and statistical tests. 

## Model formula symbols

Symbols are used differently in R model formulae:

- $+$ inclusion of variable (not addition)
- $-$ deletion of variable (not subtraction)
- $*$ inclusion of variables _and their interactions_ (not multiplication)
- $:$ interaction of variables
- $\wedge$ interaction of variables to specified degree (not exponent)

To override model symbol, use the `I()` function. 

See `help(formula)` for more information.

## Examples of model specifications

- `y ~ x`  (simple linear regression)
- `y ~ x + z` (multiple regression)
- `y ~ .` (multiple regression for all variables in data set)
- `y ~ x + z - 1` (multiple regression without intercept)
- `y ~ x + z + x:z`  (multiple regression with interaction)
- `y ~ x * z` (same as previous)
- `y ~ u + x + z + u:x + u:z + x:z + u:x:z` (multiple regression with all interactions)
- `y ~ u * x * z` (same as previous)
- `y ~ (u + x + z)^2` (multiple regression with all 2-way interactions)
- `y ~ x + I(x^2)` (polynomial regression)

Let's go to R.

## Using categorical predictors in model building

So far we have only considered numerical predictors. What about categorical predictors such as Male/Female, Democrat/Republican/Independent, Low/Medium/High, etc.?

R treats categorical predictors as _factors_.

Either define a variable as a factor in a data frame or in the `lm` formula:

- `prostate$gleason.score <- factor(prostate$gleason.score)`
- `lm(psa ~ factor(gleason.score), data=prostate)`

Recommended to define variable as factor in the data frame.

## How factors are modeled

Factors are modeled using _treatment contrasts_.

This means one level is treated as baseline and the other levels have coefficients that express change from baseline.

To make this happen R automatically codes the factor levels as dummy variables in the model matrix $\mathbf{X}$.

Say we have variable `level` with three levels: low, medium, high. R codes as follows:

```{r, echo=FALSE}
level <- factor(rep(c(1,2,3)),labels = c("low","medium","high"))
pos <- factor(rep(c(1,2,3),each=11),labels = c("left","center","right"))
test <- expand.grid(pos=pos,level=level)
contrasts(level)
```

## How factors are reported in output

The baseline is not listed per se but is pulled into the intercept. The coefficients on the other levels represent difference from baseline.


```{r, echo=FALSE}
set.seed(5)
resp <- rnorm(99, rep(c(10,20,25), each=33),)
num <- c(runif(33,1,5), runif(33,6,10), runif(33,11,15))
test <- data.frame(resp, num, test)
summary(lm(resp ~ level, test))$call
summary(lm(resp ~ level, test))$coefficients[,1, drop=F]
```


- mean "resp" for level=low is about 10
- mean "resp" for level=medium is about 10 more than "low", so 20
- mean "resp" for level=high is about 15 more than "low", so 25

## How factors work in interactions

Interacting factors with other factors yields parameter estimates for all combinations of levels.

Interacting factors with numeric variables yields a separate parameter estimate for the numeric variable at each level of the factor.

Say we have categorical predictor variable `pos` with values "right", "left" and "center". Further say we have a numeric predictor called `num`.

Specify interactions using the same formula notation:

- `resp ~ level * pos # factor:factor`
- `resp ~ level * num # factor:numeric`

## How `factor:factor` interactions are reported in output


```{r, echo=FALSE}

summary(lm(resp ~ level * pos, test))$call
summary(lm(resp ~ level * pos, test))$coefficients[,1, drop=F]
```


## How `factor:factor` interactions are interpreted

Mean response value when level=low and position=left

$10.0399$

Mean response value when level=low and position=center

$10.0399 - 0.4225 = 9.6174$

Mean response value when level=high and position=right

$10.0399 + 14.7078 + 0.6645 - 0.3188 = 25.0934$


## How `factor:numeric` interactions are reported in output

```{r, echo=FALSE}
summary(lm(resp ~ level * num, test))$call
summary(lm(resp ~ level * num, test))$coefficients[,1, drop=F]
```

## How `factor:numeric` interactions are interpreted

With one factor and one numeric, this is a _varying intercept and slope_ model. The intercept and slope vary based on the level.

Model when level=low

$9.489 + 0.217*num$

Model when level=high (note how the intercept and slope change)

$(9.489 + 13.548) + (0.217 - 0.066)*num$

This is also knows as an Analysis of Covariance (ANCOVA).


## More information on Factors

Character variables are automatically treated as factors on import (unless you specify otherwise).

How do you know if a variable is coded as factor? Use `class` or `is.factor`. Can also inspect structure of data frame using `str`.

Calling `summary` on factors produces a table of counts.

Let's go to the R script.



## Regression Diagnostics

Estimation and inference from a linear model depend on several assumptions. 

We check these assumptions using _regression diagnostics_. We assume...

- errors are independent
- errors have constant variance 
- errors are normally distributed
- all observations "fit" the model and none have large influence on the model

Violations of these assumptions can invalidate our model.

We also need to check for _collinearity_ (correlated predictor variables).

## Quick visual diagnostics using `plot`

Calling the `plot` function on the model object produces four diagnostic plots.

1. Residuals vs Fitted (_check constant variance assumption_)
2. Normal Q-Q (_check normality assumption_)
3. Scale-Location (_check constant variance assumption_)
4. Residuals vs Leverage (_check for influential observations_)

## Example of plotting model object

```{r, echo=FALSE}
row.names(mtcars) <- NULL
par(mfrow=c(2,2))
plot(lm(mpg ~ ., data=mtcars))
par(mfrow=c(1,1))
```


## How to interpret plots

1. Residuals vs Fitted: should have a horizontal line with uniform scatter of points
2. Normal Q-Q: points should lie close to diagonal line
3. Scale-Location: should have a horizontal line with uniform scatter of point; (_similar to #1 but easier to detect trend in dispersion_)
4. Residuals vs Leverage: points should lie _within_ contour lines

## Checking independence assumption

To check independence, plot residuals against...

- time variables present (e.g., order of observation)
- spatial variables 
- predictors used in the model 

A pattern that is not random suggests lack of independence.


## Measures of influence

Calling the `influence.measures` function on a model object produces a table of various influence measures, including DFBETAS, DFFITS, covariance ratios, Cook's distances, and hat matrix values.

Calling `summary` on a saved `influence.measures` object will identify influential cases.

Cases which are influential with respect to any of these measures are marked with an asterisk.

```
im.out <- influence.measures(model.object)
summary(im.out)
```

## The meanings of "measure of influence"

- DFBETAS: measures influence that case _i_ has on each regression coefficient
- DFFITS: measures influence that case _i_ has on fitted value $\hat{y_{i}}$
- covariance ratios: measures influence of case _i_ on standard errors of the regression coefficients
- Cook's distance: measures influence that case _i_ has on all fitted values 
- hat matrix: measures _leverage_ of case _i_ (i.e. outlying predictor variables)

Each of these have thresholds that when exceeded suggest an influential case. Let R remember those for you.

## Checking for collinearity

_Variance Inflation Factors_ allow us to detect collinearity.

The `vif` function from the `car` package makes this easy.

`vif(model.object)` returns VIF values.

If any greater than 10, that's an indication of collinearity.

Let's go to R.

## Updating linear models

After fitting a model, looking at coefficients and their standard errors, and examining diagnostics, we frequently need to update the model.

This means removing or adding predictors and/or removing observations.

Adding/removing predictors can be accomplished with the `update` function.

Removing observations can be accomplished with the `subset=` argument on the `lm` function


## Model selection

The task of adding or removing predictors is often referred to as _model selection_ or _variable selection_.

Reasons for using a subset of predictors instead of all of them:

1. Simplicity. The simplest explanation is the best.
2. Better precision. Unnecessary predictors add noise.
3. Avoiding Collinearity. Can hide relationships.
4. Future efficiency. Save time and money not measuring redundant predictors


## Two main types of model selection

1. Testing-based approach: compare successive models using hypotheis tests
2. Criterion-based approach: optimize a measure of goodness

Which to choose? That's up to you. Whatever you do, expect to do some experimentation and iteration to find better models.

Also expect to use a great deal of subjective judgment. 


## Testing-based approach

Compare nested models using `anova` function.

For example:
```
lm1 <- lm(y ~ x1 + x2 + x3 + x4)
lm2 <- update(lm1, . ~ . - x3 - x4) 
anova(lm2, lm1)
```

Null hypothesis: both models the same (smaller model fits just as well as bigger model).

A low p-value says reject null; the larger model has more explanatory power.


## Criterion-based approach - AIC

One common criteria is the Akaike Information Criterion (AIC).

AIC is defined as $-2$ max loglikelihood $+ 2p$. We want to minimize AIC.

This approach requires _no hypothesis testing_ unlike the testing-based procedure. 

Use the `step` function in R to execute a search method that compares models sequentially.

Back to the R script.

## Logistic Regression

Sometimes our response is binary: yes/no, success/failure, diseased/non-diseased, etc. Usually coded as 1 and 0.

We use logistic regression to model a binary response.

Logistic regression is a type of _generalized linear model_ (GLM).

We fit GLM models in R using the `glm()` function. It works like the `lm()` function except we specify which GLM to fit using the `family=` argument.

Logistic regression requires `family=binomial`.

## Interpreting Logistic Regression coefficients

Logistic regression coefficients give the change in the _log odds_ of the outcome for a one unit increase in the predictor variable.

Exponentiate coefficient to get the _odds ratio_.

Example: say logistic regression coefficient of $X1$ is 1.06. $exp(1.06) = 2.89$, which says that for a one unit increase in X1 the odds of response = 1 is 2.89 times higher than for  response = 0.  

odds = $p / (1-p)$

odds ratio = $\frac{ p_{1} / (1 - p_{1}) }{ p_{2} / (1 - p_{2}) }$

Let's go to R.

## References

Faraway, J. (2005). _Linear Models in R_. London: Chapman & Hall.

Fox, J. (2002). _A R and S-Plus Companion to Applied Regression_. London: Sage.

James, G., et al. (2013). _An Introduction to Statistical Learning_. New York: Springer.

Kutner, M., et al. (2005) _Applied Linear Statistical Models_ (5th ed.). New York: McGraw-Hill.

Maindonald J., Braun, J.W. (2010). _Data Analysis and Graphics Using R_ (3rd ed.). Cambridge: Cambridge Univ Press.


## StatLab

Thanks for coming today!

For help and advice with your data analysis, contact the StatLab to set up an appointment: statlab@virginia.edu

Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/statlab/

Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/
