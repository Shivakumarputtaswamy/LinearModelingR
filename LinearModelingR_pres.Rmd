---
title: "Linear Modeling in R"
author: "Clay Ford"
date: "October 2014"
output: beamer_presentation
---

## What are Linear Models?

Linear Models are mathematical representations of the process that (_we think_) gave rise to our data.

The model seeks to explain the relationship between a variable of interest, our _response_ or _dependent_ variable, and one or more _predictor_ or _independent_ variables.

Linear models feature _linear parameters_. That means no parameter appears as an exponent or is multiplied or divided by another parameter. Parameters are _additive_.

Linear often refers to straight lines, but linear models can be curved.

## Example of a classic linear model

$$Y_{i} = \beta_{0} +  \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i}$$

- $Y_{i}$ is the response of the $i^{th}$ observation
- $\beta_{0}, \beta_{1}, \beta_{2}$ are coefficients
- $X_{i1}$ and $X_{i2}$ are predictors of the $i^{th}$ observation
- $\epsilon_{i}$ is random error of the $i^{th}$ observation; we assume errors are independent and come from a Normal distribution with mean = 0 and variance $\sigma^{2}$; this implies $Y_{i}$ comes from a Normal distribution with mean $\beta_{0} +  \beta_{1}X_{1} + \beta_{2}X_{2}$ and variance $\sigma^{2}$.

_Building a linear model_ means we propose a linear model and then estimate the parameters and the variance of the error term. Above, this means estimating $\beta_{0}, \beta_{1}, \beta_{2}$ and $\sigma^{2}$. This is what we do in R.

## Matrix representation of linear models

Linear models are often represented as matrices/vectors.

$$Y = X\beta + \epsilon$$

For our previous example:

$$\left( \begin{array}{c} y_{1} \\ y_{2} \\ \dots \\ y_{n} \end{array} \right) =   
  \left( \begin{array}{ccc} 1 & x_{11} & x_{12} \\
                          1 & x_{21} & x_{22} \\
                      \dots & \dots & \dots \\
                      1 & x_{n1} & x_{n2} \end{array} \right) 
  \left( \begin{array}{c} \beta_{0} \\ \beta_{1} \\ \beta_{2} \end{array} \right) +
  \left( \begin{array}{c} \epsilon_{1} \\ \epsilon_{2} \\ \dots \\ \epsilon_{n} \end{array} \right)$$

The column of ones incorporates the intercept term. R makes it easy to visualize this representation.

## Estimating $\mathbf{\beta}$

The best estimate minimizes the sum of squared errors. 2D example: line with intercept $\hat{\beta_{0}}$ and slope $\hat{\beta_{1}}$ with squared and summed distances of red lines (ie, residuals) as small as possible.

```{r, echo=FALSE}
x <- seq(1,10)
set.seed(1)
y <- 3 + 2*x + rnorm(n=10,mean=0,sd=3)
opar <- par(mai=c(1.093333, 1.093333, 0, 0.560000))
plot(x,y, xlim=c(0,10), ylim=c(0,25))
slm <- lm(y ~ x)
abline(slm)
segments(x0=x, y0=slm$fitted, x1=x, y1=y, col="red")
par(opar)

```


## Estimating $\sigma^{2}$

Square and sum the red lines (sum of squared errors, aka residual sum of squares, RSS) and divide by $n - p$. ($p =$ number of coefficients) 

```{r, echo=FALSE}
x <- seq(1,10)
set.seed(1)
y <- 3 + 2*x + rnorm(n=10,mean=0,sd=3)
opar <- par(mai=c(1.093333, 1.093333, 0, 0.560000))
plot(x,y, xlim=c(0,10), ylim=c(0,25))
slm <- lm(y ~ x)
abline(slm)
segments(x0=x, y0=slm$fitted, x1=x, y1=y, col="red")
par(opar)
```

## Before fitting a linear model: import data

Most any kind of data can be read into R. The usual form of the function is `read.x`.

- CSV: `read.csv(file="file.csv")`
- TXT: `read.table(file="file.txt", header=TRUE)`
- Fixed-width: `read.fwf(file="file.dat", widths = c(1,2,3))`

The `foreign` package allows you to read in data from other programs. `library(foreign)`:

- Stata: `read.dta`
- SPSS: `read.spss`
- Minitab: `read.mtp`

You can also use point-and-click in R Studio: "Import Dataset" button.

## Before fitting a linear model: descriptive stats

Look at descriptive statistics before building a model. Helps you...

- spot missing data
- identify potential errors
- determine if data coded correctly

Four basic functions:

1. `summary(yourdata)`: Statistical summaries of all variables
2. `str(yourdata)`: Structure of data 
3. `head(yourdata)`: First six records 
4. `tail(yourdata)`: Last six records


## Before fitting a linear model: exploratory plots

Always try to visually examine your data before building a model. Helps you...

- identify potential models
- spot potential outliers or errors
- determine if transformations may be necessary

Three basic plots:

1. `hist(response)`: Histogram of response 
2. `plot(response ~ predictor, data=yourdata)`: scatterplot/boxplots
3. `pairs(yourdata)`: All pairwise scatter plots: 


## Fitting a linear model in R

The basic function is `lm`. The main arguments are `formula` and `data`. 

The "formula" is the linear model expressed in what is called _Wilkinson-Rogers_ notation. 

To fit $Y_{i} = \beta_{0} +  \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i}$ do the following:  
`lm(formula=response ~ var1 + var2, data=your_dataset)`

Or more concisely:  
`lm(response ~ var1 + var2, your_dataset)`

## Saving and working with a linear model

Results of a linear model can be saved, like so:  
`lm1 <- lm(Y ~ X1 + X2, your_dataset)`

`lm1` is a linear model object that contains various quantities of interest.

Use _extractor_ functions to view the different quantities. Common ones are `summary`, `coef`, `residuals`, `fitted`, `deviance` (returns RSS), and `df.residual`.

For example, `summary(lm1)`.

Let's go to the R script.

## A closer look at the R model summary

```
Call:
lm(formula = psa ~ volume + weight + age + bph + svi + cap.pen + 
    gleason.score, data = prostate)
```
Repeat of the function call.  Useful if result is saved and then printed later.

```
Residuals:
     Min       1Q   Median       3Q      Max 
-1.88309 -0.46629  0.08045  0.47380  1.53219 
```

Quick check of the distributional (Normal) assumptions of residuals. Median should not be far from 0. Max and Min should be roughly equal in absolute value.

## A closer look at the R model summary

```
Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -0.685796   0.998754  -0.687  0.49409    
volume         0.069454   0.014624   4.749 7.77e-06 ***
...
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

```
- **Estimate**: $\hat{\beta}$
- **Std. Error**: SE of the $\hat{\beta}$ 
- **t value**: statistic for hypothesis test $H_{0}:\hat{\beta} = 0$ (Estimate $\div$ Std. Error)
- **Pr(>|t|)**: p-value of hypothesis test (2-sided)
- **Signif. codes**: indicators of significance; one star means $0.01 < p < 0.05$

## A closer look at the R model summary

```
Residual standard error: 0.7679 on 89 degrees of freedom
Multiple R-squared:  0.5893,  Adjusted R-squared:  0.557 
F-statistic: 18.24 on 7 and 89 DF,  p-value: 7.694e-15
```
- **Residual standard error**: $\hat{\sigma}$
- **degrees of freedom**: # of obs - # of predictors
- **Multiple R-squared**: measure of model fit
- **Adjusted R-squared**: measure of model fit adjusted for number of parameters
- **F-statistic**: statistic for hypothesis test all predictor parameters = 0
- **p-value**: p-value of hypothesis test


## Confidence intervals for $\hat{\beta}$

Confidence intervals allow us to express uncertainty in our estimates.

They tell us about plausible values for parameters that hypothesis tests cannot.

To extract from model in R, use `confint` function. Output includes lower and upper bounds.

Default is 95%, but can be modified using the `level` argument.

The `arm` package provides a nice function called `coefplot` to easily plot confidence intervals of coefficients.

## Confidence intervals for predictions

We can make predictions using our linear model, but there will be uncertainty. We need to calculate that uncertainty.

Two forms of prediction:

1. predictions of mean response (predicted mean PSA given these predictors)
2. predictions of observed response (predicted PSA value for _an individual_ with given predictors)

To make predictions in R, use `predict` function. Output includes fit and lower/upper bounds.

- use argument `interval="confidence"` for mean response
- use argument `interval="predicted"` for observed response

Let's go to R.

## Model specification in R

We mentioned earlier R uses the _Wilkinson-Rogers_ notation for specifying models:

> response variable ~ explanatory variable(s)

The tilde (~) can be read as "is modelled as a function of" or "described by" or "regressed on".

Model formulation is used throughout R in plotting, aggregation and statistical tests. 

## Model formula symbols

Symbols are used differently in R model formulae:

- $+$ inclusion of variable (not addition)
- $-$ deletion of variable (not subtraction)
- $*$ inclusion of variables _and their interactions_ (not multiplication)
- $:$ interaction of variables
- $\wedge$ interaction of variables to specified degree (not exponent)

To override model symbol, use the `I()` function. 

See `help(formula)` for more information.

## Examples of model specifications

- `y ~ x`  (simple linear regression)
- `y ~ x + z` (multiple regression)
- `y ~ .` (multiple regression for all variables in data set)
- `y ~ x + z - 1` (multiple regression without intercept)
- `y ~ x + z + x:z`  (multiple regression with interaction)
- `y ~ x * z` (same as previous)
- `y ~ u + x + z + u:x + u:z + x:z + u:x:z` (multiple regression with all interactions)
- `y ~ u * x * z` (same as previous)
- `y ~ (u + x + z)^2` (multiple regression with all 2-way interactions)
- `y ~ x + I(x^2)` (polynomial regression)

Let's go to R.

## Regression Diagnostics

Estimation and inference from a linear model depend on several assumptions. 

We check these assumptions using _regression diagnostics_. We assume...

- errors are independent
- errors have constant variance 
- errors are normally distributed
- all observations "fit" the model and none have large influence on the model

Violations of these assumptions can invalidate our model.

We also need to check for _collinearity_ (correlated predictor variables).

## Quick visual diagnostics using `plot`

Calling the `plot` function on the model object produces four diagnostic plots.

1. Residuals vs Fitted (_check constant variance assumption_)
2. Normal Q-Q (_check normality assumption_)
3. Scale-Location (_check constant variance assumption_)
4. Residuals vs Leverage (_check for influential observations_)

## Example of plotting model object

```{r, echo=FALSE}
row.names(mtcars) <- NULL
par(mfrow=c(2,2))
plot(lm(mpg ~ ., data=mtcars))
par(mfrow=c(1,1))
```


## How to interpret plots

1. Residuals vs Fitted: should have a horizontal line with uniform scatter of points
2. Normal Q-Q: points should lie close to diagonal line
3. Scale-Location: should have a horizontal line with uniform scatter of point; (_similar to #1 but easier to detect trend in dispersion_)
4. Residuals vs Leverage: points should lie _within_ contour lines

## Checking independence assumption

To check independence, plot residuals against...

- time variables present (e.g., order of observation)
- spatial variables 
- predictors used in the model 

A pattern that is not random suggests lack of independence.


## Measures of influence

Calling the `influence.measures` function on a model object produces a table of various influence measures, including DFBETAS, DFFITS, covariance ratios, Cook's distances, and hat matrix values.

Calling `summary` on a saved `influence.measures` object will identify influential cases.

Cases which are influential with respect to any of these measures are marked with an asterisk.

```
im.out <- influence.measures(model.object)
summary(im.out)
```

## The meanings of "measure of influence"

- DFBETAS: measures influence that case _i_ has on each regression coefficient
- DFFITS: measures influence that case _i_ has on fitted value $\hat{y_{i}}$
- covariance ratios: measures influence of case _i_ on standard errors of the regression coefficients
- Cook's distance: measures influence that case _i_ has on all fitted values 
- hat matrix: measures _leverage_ of case _i_ (i.e. outlying predictor variables)

Each of these have thresholds that when exceeded suggest an influential case. Let R remember those for you.

## Checking for collinearity

_Variance Inflation Factors_ allow us to detect collinearity.

The `vif` function from the `faraway` package makes this easy.

`vif(model.object)` returns VIF values.

If any greater than 10, that's an indication of collinearity.

Let's go to R.

## Updating linear models

After fitting a model, looking at coefficients and their standard errors, and examining diagnostics, we frequently need to update the model.

This means removing or adding predictors and/or removing observations.

Adding/removing predictors can be accomplished with the `update` function.

Removing observations can be accomplished with the `subset=` argument on the `lm` function


## Model selection

The task of adding or removing predictors is often referred to as _model selection_ or _variable selection_.

Reasons for using a subset of predictors instead of all of them:

1. Simplicity. The simplest explanation is the best.
2. Better precision. Unnecessary predictors add noise.
3. Avoiding Collinearity. Can hide relationships.
4. Future efficiency. Save time and money not measuring redundant predictors


## Two main types of  model selection

1. Testing-based approach: compare successive models using hypotheis tests
2. Criterion-based approach: optimize a measure of goodness

Which to choose? That's up to you. Whatever you do, expect to do some experimentation and iteration to find better models.

Also expect to use a great deal of subjective judgment. 


## Testing-based approach

Compare nested models using `anova` function.

For example:
```
lm1 <- lm(y ~ x1 + x2 + x3 + x4)
lm2 <- update(lm1, . ~ . - x3 - x4) 
anova(lm2, lm1)
```

Null hypothesis: both models the same (smaller model fits just as well as bigger model).

A low p-value says reject null; the larger model has more explanatory power.


## Criterion-based approach - AIC

One common criteria is the Akaike Information Criterion (AIC).

AIC is defined as $-2$ max loglikelihood $+ 2p$. We want to minimize AIC.

This approach require _no hypothesis testing_ unlike the testing-based procedure. 

Use the `step` function in R to execute a search method that compares models sequentially.


## Criterion-based approach - BIC

Another criteria is Schwarz's Bayesian criterion (BIC).

BIC is defined as $-2$ max loglikelihood $+ log(n)p$. We want to minimize BIC.

BIC places a heavier penalty on models with many predictors.

Use the `regsubsets` function from the `leaps` package to find the model with lowest BIC.


## Criterion-based approach - Best Subset Selection

1. Let $\mathcal{M}_{0}$ be the null model with no predictors. That is, it predicts the sample mean for each observation.
2. For $k = 1,2,...,p$ 
    + fit a model for all possible combinations of $k$ predictors.
    + pick best model (ie, one with smallest RSS) and call it $\mathcal{M}_{k}$.
3. Select a single best model from among $\mathcal{M}_{0},...,\mathcal{M}_{p}$ using a criterion such as BIC.

```
library(leaps)
out <- regsubsets(response ~ ., data=yourdata)
plot(out)
```
 
## Criterion-based approach - Forward Selection

1. Let $\mathcal{M}_{0}$ be the null model with no predictors. That is, it predicts the sample mean for each observation.
2. For $k = 0,1,...,p-1$
    + consider all $p-k$ models that augment the predictors in $\mathcal{M}_{k}$ with one additional predictor.
    + pick best model (ie, one with smallest RSS) and call it $\mathcal{M}_{k+1}$
3. Select a single best model from among $\mathcal{M}_{0},...,\mathcal{M}_{p}$ using a criterion such as BIC.


```
library(leaps)
out <- regsubsets(response ~ ., data=yourdata, 
                  method="forward")
plot(out)
```

## Criterion-based approach - Backward Selection

1. Let $\mathcal{M}_{p}$ be the full model with _all_ predictors.
2. For $k = p, p-1,...,1$ 
    + fit all $k$ models that contain all but one of the predictors in $\mathcal{M}_{k}$;
    + pick best model (ie, one with smallest RSS) and call it $\mathcal{M}_{k-1}$
3. Select a single best model from among $\mathcal{M}_{0},...,\mathcal{M}_{p}$ using a criterion such as BIC.


```
library(leaps)
out <- regsubsets(response ~ ., data=yourdata, 
                  method="backward")
plot(out)
```
Let's go to R.



## Using categorical predictors in model building

So far we have only considered numerical predictors. What about categorical predictors such as Male/Female, Democrat/Republican/Independent, Low/Medium/High, etc.?

R treats categorical predictors as _factors_.

Either define a variable as a factor in a data frame or in the `lm` formula:

- `prostate$gleason.score <- factor(prostate$gleason.score)`
- `lm(psa ~ factor(gleason.score), data=prostate)`

Recommended to define variable as factor in the data frame.

## How factors are modeled

Factors are modeled using _treatment contrasts_.

This means one level is treated as baseline and the other levels have coefficients that express change from baseline.

To make this happen R automatically codes the factor levels as dummy variables in the model matrix $\mathbf{X}$.

Say we have variable `level` with three levels: low, medium, high. R codes as follows:

```{r, echo=FALSE}
level <- factor(rep(c(1,2,3)),labels = c("low","medium","high"))
pos <- factor(rep(c(1,2,3),each=11),labels = c("left","center","right"))
test <- expand.grid(pos=pos,level=level)
contrasts(level)
```

## How factors are reported in output

The baseline is not listed per se but is pulled into the intercept. The coefficients on the other levels represent difference from baseline.


```{r, echo=FALSE}
set.seed(5)
resp <- rnorm(99, rep(c(10,20,25), each=33),)
num <- c(runif(33,1,5), runif(33,6,10), runif(33,11,15))
test <- data.frame(resp, num, test)
summary(lm(resp ~ level, test))$call
summary(lm(resp ~ level, test))$coefficients[,1, drop=F]
```


- mean "resp" for level=low is about 10
- mean "resp" for level=medium is about 10 more than "low", so 20
- mean "resp" for level=high is about 15 more than "low", so 25

## How factors work in interactions

Interacting factors with other factors yields parameter estimates for all combinations of levels.

Interacting factors with numeric variables yields a separate parameter estimate for the numeric variable at each level of the factor.

Say we have categorical predictor variable `pos` with values "right", "left" and "center". Further say we have a numeric predictor called `num`.

Specify interactions using the same formula notation:

- `resp ~ level * pos # factor:factor`
- `resp ~ level * num # factor:numeric`

## How `factor:factor` interactions are reported in output


```{r, echo=FALSE}

summary(lm(resp ~ level * pos, test))$call
summary(lm(resp ~ level * pos, test))$coefficients[,1, drop=F]
```


## How `factor:factor` interactions are interpreted

Mean response value when level=low and position=left

$10.0399$

Mean response value when level=low and position=center

$10.0399 - 0.4225 = 9.6174$

Mean response value when level=high and position=right

$10.0399 + 14.7078 + 0.6645 - 0.3188 = 25.0934$


## How `factor:numeric` interactions are reported in output

```{r, echo=FALSE}
summary(lm(resp ~ level * num, test))$call
summary(lm(resp ~ level * num, test))$coefficients[,1, drop=F]
```

## How `factor:numeric` interactions are interpreted

With one factor and one numeric, this is a _varying intercept and slope_ model. The intercept and slope vary based on the level.

Model when level=low

$9.489 + 0.217*num$

Model when level=high (note how the intercept and slope change)

$(9.489 + 13.548) + (0.217 - 0.066)*num$

This is also knows as an Analysis of Covariance (ANCOVA).


## More information on Factors

Character variables are automatically treated as factors on import (unless you specify otherwise).

How do you know if a variable is coded as factor? Use `class` or `is.factor`. Can also inspect structure of data frame using `str`.

Calling `summary` on factors produces a table of counts.

Let's go to the R script.


## Model validation

When using a linear model for prediction, we'd like to know something about the model's accuracy.

We therefore estimate the _test error rate_: the average error that results from using a model to predict responses on _new_ observations.

Basic process: 

1. _Randomly_ split data into two groups, one to create model (_training_ set) and one to test it (_testing_ set)
2. Build model with training data
3. Make predictions using testing data
4. Calculate the test error rate

## Mean Square Error (MSE)

In regression, one way to measure test error is the _mean square error (MSE)_:

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}$$

In words: "the mean of the squared residuals"

It's important we calculate this using _test_ data on a model built with _training_ data.

The test error calculated with same data used to build model is too optimistic.

## The validation set approach

An easy way to split data into training and testing sets is to use the `sample` function:

```{r}
sample(10,5)
```

Use `sample` to create a vector of random row numbers and then use vector in the `subset=` argument of `lm`. 

Split a data frame called "d" in half and build model:
```
train <- sample(nrow(d), round(0.5*nrow(d)))
m <- lm(resp ~ pred, data=d, subset=train)
```

## _k_-fold cross validation

Randomly divide data into _k_ groups, or folds, of approximately equal size. 

Each group is used as a test set for a model built with the other groups, resulting in _k_ MSE values.

The mean of the _k_ MSE values is the test error estimate.

We can use the `cv.glm` function in the `boot` package to do this for us.

Let's go to the R script.

## References

Faraway, J. (2005). _Linear Models in R_. London: Chapman & Hall.

James, G., et al. (2013). _An Introduction to Statistical Learning_. New York: Springer.

Kutner, M., et al. (2005) _Applied Linear Statistical Models_ (5th ed.). New York: McGraw-Hill.

